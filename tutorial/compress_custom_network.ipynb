{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: compress and run custom network"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This brief tutorial shows how to compress a custom network with EfficientBioAI and do the inference.\n",
    "- Model: naive 2d unet picked from:[github](https://github.com/milesial/Pytorch-UNet/blob/master/unet/unet_model.py)\n",
    "- data: [Simulated nuclei of HL60 cells stained with Hoescht](http://celltrackingchallenge.net/2d-datasets/)\n",
    "- Compression strategy: L2 Norm Prune and QAT int8 quantization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our package just focus on the compression part, and have no idea what is about the pre-processing of dataset and the logic of train/infer the data, users need to provide the following info for the compression:\n",
    "\n",
    "- a calibration dataloader containing several images;\n",
    "- the training api, which is used to do the fine-tuning of the compressed model; \n",
    "- the inference api, which is used to do the calibration during the quantization step.\n",
    "\n",
    "After providing these logics, users can use our package to compress the model and do the inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ISAS.DE/yu.zhou/miniconda3/envs/yz_deployment/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from model.unet import Unet\n",
    "from tqdm.contrib import tenumerate\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We don't have the pretrained model, so need to train it from scratch:\n",
    "!wget http://data.celltrackingchallenge.net/training-datasets/Fluo-N2DH-SIM+.zip -P ./data\n",
    "!unzip ./data/Fluo-N2DH-SIM+.zip -d ./data\n",
    "!rm ./data/Fluo-N2DH-SIM+.zip\n",
    "!python train_unet.py --data_path \"./data/Fluo-N2DH-SIM+/02\" --gt_path \"./data/Fluo-N2DH-SIM+/02_GT/SEG\" --num_epoch 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the seed for reproducibility:\n",
    "from monai.utils import set_determinism\n",
    "\n",
    "seed_value = 2023\n",
    "torch.manual_seed(seed_value)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "set_determinism(seed=seed_value)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Compress the model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Load the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict = torch.load(\"./unet.pth\")\n",
    "net = Unet(in_channels=1, classes=2)\n",
    "net.load_state_dict(state_dict)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Some logics required to be provided by users:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-05-04 17:41:43,662 - Resource 'XMLSchema.xsd' is already loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ISAS.DE/yu.zhou/miniconda3/envs/yz_deployment/lib/python3.8/site-packages/monai/utils/deprecate_utils.py:107: FutureWarning: <class 'monai.transforms.utility.array.AddChannel'>: Class `AddChannel` has been deprecated since version 0.8. please use MetaTensor data type and monai.transforms.EnsureChannelFirst instead.\n",
      "  warn_deprecated(obj, msg, warning_category)\n",
      "150it [00:00, 327339.02it/s]\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "from monai.data import DataLoader, Dataset\n",
    "from custom import train, infer\n",
    "from data import generate_data_dict, train_transform, test_transform\n",
    "import yaml\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# 1. train logic and infer logic:\n",
    "fine_tune = partial(train, num_epoch=1)\n",
    "calibrate = partial(infer, calib_num=4)\n",
    "\n",
    "# 2. Iterable data, here is a dataloader, used for calibration and fine-tuning:\n",
    "train_data_path = Path(\"./data/Fluo-N2DH-SIM+/02\")\n",
    "train_gt_path = Path(\"./data/Fluo-N2DH-SIM+/02_GT/SEG\")\n",
    "dataset = Dataset(\n",
    "    data=generate_data_dict(train_data_path, train_gt_path), transform=train_transform\n",
    ")\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True, num_workers=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Compress the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from efficientbioai.compress_ppl import Pipeline\n",
    "from efficientbioai.utils.misc import Dict2ObjParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_path = Path(\"./custom_config.yaml\")\n",
    "with open(cfg_path, \"r\") as stream:\n",
    "    config_yml = yaml.safe_load(stream)\n",
    "    config = Dict2ObjParser(config_yml).parse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-05-04 18:37:18 [EFFICIENTBIOAI] INFO: start to compress: quantize: False, prune: False, backend: openvino, model_name: academic\n",
      "2023-05-04 18:37:18 [EFFICIENTBIOAI] INFO: Start quantization...\n",
      "[MQBENCH] INFO: Quantize model Scheme: BackendType.OPENVINO Mode: Training\n",
      "[MQBENCH] INFO: Weight Quant Scheme is overrided!\n",
      "[MQBENCH] INFO: Activation Quant Scheme is overrided!\n",
      "[MQBENCH] INFO: Weight Qconfig:\n",
      "    FakeQuantize: LearnableFakeQuantize Params: {}\n",
      "    Oberver:      EMAQuantileObserver Params: Symmetric: True / Bitwidth: 4 / Per channel: False / Pot scale: False / Extra kwargs: {}\n",
      "[MQBENCH] INFO: Activation Qconfig:\n",
      "    FakeQuantize: LearnableFakeQuantize Params: {}\n",
      "    Oberver:      EMAQuantileObserver Params: Symmetric: True / Bitwidth: 4 / Per channel: False / Pot scale: False / Extra kwargs: {}\n",
      "[MQBENCH] INFO: Replace module to qat module.\n",
      "[MQBENCH] INFO: Now all weight quantizers will effectively use only 7 bits out of 8 bits. This resolves the overflow issue problem on AVX2 and AVX-512 machines.\n",
      "[MQBENCH] INFO: Insert act quant x_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Set inc_conv_conv_0 post act quantize to 8 bit unsigned type.\n",
      "[MQBENCH] INFO: Insert act quant inc_conv_conv_0_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Set inc_conv_conv_3 post act quantize to 8 bit unsigned type.\n",
      "[MQBENCH] INFO: Insert act quant inc_conv_conv_3_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Set down1_mpconv_1_conv_0 post act quantize to 8 bit unsigned type.\n",
      "[MQBENCH] INFO: Insert act quant down1_mpconv_1_conv_0_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Set down1_mpconv_1_conv_3 post act quantize to 8 bit unsigned type.\n",
      "[MQBENCH] INFO: Insert act quant down1_mpconv_1_conv_3_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Set down2_mpconv_1_conv_0 post act quantize to 8 bit unsigned type.\n",
      "[MQBENCH] INFO: Insert act quant down2_mpconv_1_conv_0_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Set down2_mpconv_1_conv_3 post act quantize to 8 bit unsigned type.\n",
      "[MQBENCH] INFO: Insert act quant down2_mpconv_1_conv_3_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Set down3_mpconv_1_conv_0 post act quantize to 8 bit unsigned type.\n",
      "[MQBENCH] INFO: Insert act quant down3_mpconv_1_conv_0_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Set down3_mpconv_1_conv_3 post act quantize to 8 bit unsigned type.\n",
      "[MQBENCH] INFO: Insert act quant down3_mpconv_1_conv_3_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Set down4_mpconv_1_conv_0 post act quantize to 8 bit unsigned type.\n",
      "[MQBENCH] INFO: Insert act quant down4_mpconv_1_conv_0_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Set up1_conv_conv_0 post act quantize to 8 bit unsigned type.\n",
      "[MQBENCH] INFO: Insert act quant up1_conv_conv_0_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Set up2_conv_conv_0 post act quantize to 8 bit unsigned type.\n",
      "[MQBENCH] INFO: Insert act quant up2_conv_conv_0_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Set up3_conv_conv_0 post act quantize to 8 bit unsigned type.\n",
      "[MQBENCH] INFO: Insert act quant up3_conv_conv_0_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Set up4_conv_conv_0 post act quantize to 8 bit unsigned type.\n",
      "[MQBENCH] INFO: Insert act quant up4_conv_conv_0_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Set up4_conv_conv_3 post act quantize to 8 bit unsigned type.\n",
      "[MQBENCH] INFO: Insert act quant up4_conv_conv_3_post_act_fake_quantizer\n",
      "[MQBENCH] INFO: Enable observer and Disable quantize.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 4/75 [00:09<02:54,  2.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MQBENCH] INFO: Disable observer and Enable quantize.\n",
      "[MQBENCH] INFO: Merge BN for deploy.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MQBENCH] INFO: Export to onnx.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: The shape inference of ::LearnablePerTensorAffine type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of ::LearnablePerTensorAffine type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of ::LearnablePerTensorAffine type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of ::LearnablePerTensorAffine type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of ::LearnablePerTensorAffine type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of ::LearnablePerTensorAffine type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of ::LearnablePerTensorAffine type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of ::LearnablePerTensorAffine type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of ::LearnablePerTensorAffine type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of ::LearnablePerTensorAffine type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of ::LearnablePerTensorAffine type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of ::LearnablePerTensorAffine type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of ::LearnablePerTensorAffine type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of ::LearnablePerTensorAffine type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of ::LearnablePerTensorAffine type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of ::LearnablePerTensorAffine type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of ::LearnablePerTensorAffine type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of ::LearnablePerTensorAffine type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of ::LearnablePerTensorAffine type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of ::LearnablePerTensorAffine type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of ::LearnablePerTensorAffine type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of ::LearnablePerTensorAffine type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of ::LearnablePerTensorAffine type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of ::LearnablePerTensorAffine type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of ::LearnablePerTensorAffine type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of ::LearnablePerTensorAffine type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of ::LearnablePerTensorAffine type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of ::LearnablePerTensorAffine type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of ::LearnablePerTensorAffine type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of ::LearnablePerTensorAffine type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of ::LearnablePerTensorAffine type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of ::LearnablePerTensorAffine type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of ::LearnablePerTensorAffine type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of ::LearnablePerTensorAffine type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of ::LearnablePerTensorAffine type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of ::LearnablePerTensorAffine type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of ::LearnablePerTensorAffine type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MQBENCH] INFO: Extract qparams for OPENVINO.\n",
      "[MQBENCH] INFO: Finish deploy process.\n",
      "2023-05-04 18:37:29 [EFFICIENTBIOAI] INFO: Quantization finished!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: The shape inference of ::LearnablePerTensorAffine type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of ::LearnablePerTensorAffine type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of ::LearnablePerTensorAffine type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of ::LearnablePerTensorAffine type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of ::LearnablePerTensorAffine type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of ::LearnablePerTensorAffine type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of ::LearnablePerTensorAffine type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of ::LearnablePerTensorAffine type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of ::LearnablePerTensorAffine type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of ::LearnablePerTensorAffine type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of ::LearnablePerTensorAffine type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of ::LearnablePerTensorAffine type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of ::LearnablePerTensorAffine type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of ::LearnablePerTensorAffine type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of ::LearnablePerTensorAffine type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of ::LearnablePerTensorAffine type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of ::LearnablePerTensorAffine type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of ::LearnablePerTensorAffine type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of ::LearnablePerTensorAffine type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of ::LearnablePerTensorAffine type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of ::LearnablePerTensorAffine type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of ::LearnablePerTensorAffine type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of ::LearnablePerTensorAffine type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of ::LearnablePerTensorAffine type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of ::LearnablePerTensorAffine type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of ::LearnablePerTensorAffine type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of ::LearnablePerTensorAffine type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of ::LearnablePerTensorAffine type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of ::LearnablePerTensorAffine type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of ::LearnablePerTensorAffine type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of ::LearnablePerTensorAffine type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of ::LearnablePerTensorAffine type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of ::LearnablePerTensorAffine type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of ::LearnablePerTensorAffine type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of ::LearnablePerTensorAffine type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of ::LearnablePerTensorAffine type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of ::LearnablePerTensorAffine type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of ::LearnablePerTensorAffine type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of ::LearnablePerTensorAffine type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of ::LearnablePerTensorAffine type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of ::LearnablePerTensorAffine type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of ::LearnablePerTensorAffine type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of ::LearnablePerTensorAffine type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of ::LearnablePerTensorAffine type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of ::LearnablePerTensorAffine type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of ::LearnablePerTensorAffine type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of ::LearnablePerTensorAffine type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of ::LearnablePerTensorAffine type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of ::LearnablePerTensorAffine type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of ::LearnablePerTensorAffine type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of ::LearnablePerTensorAffine type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of ::LearnablePerTensorAffine type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of ::LearnablePerTensorAffine type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of ::LearnablePerTensorAffine type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of ::LearnablePerTensorAffine type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of ::LearnablePerTensorAffine type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of ::LearnablePerTensorAffine type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of ::LearnablePerTensorAffine type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of ::LearnablePerTensorAffine type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of ::LearnablePerTensorAffine type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of ::LearnablePerTensorAffine type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of ::LearnablePerTensorAffine type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of ::LearnablePerTensorAffine type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of ::LearnablePerTensorAffine type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of ::LearnablePerTensorAffine type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of ::LearnablePerTensorAffine type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of ::LearnablePerTensorAffine type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of ::LearnablePerTensorAffine type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of ::LearnablePerTensorAffine type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of ::LearnablePerTensorAffine type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of ::LearnablePerTensorAffine type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of ::LearnablePerTensorAffine type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of ::LearnablePerTensorAffine type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of ::LearnablePerTensorAffine type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of ::LearnablePerTensorAffine type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of ::LearnablePerTensorAffine type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of ::LearnablePerTensorAffine type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check for a new version of Intel(R) Distribution of OpenVINO(TM) toolkit here https://software.intel.com/content/www/us/en/develop/tools/openvino-toolkit/download.html?cid=other&source=prod&campid=ww_2023_bu_IOTG_OpenVINO-2022-3&content=upg_all&medium=organic or on https://github.com/openvinotoolkit/openvino\n",
      "[ INFO ] The model was converted to IR v11, the latest model format that corresponds to the source DL framework input/output format. While IR v11 is backwards compatible with OpenVINO Inference Engine API v1.0, please use API v2.0 (as of 2022.1) to take advantage of the latest improvements in IR v11.\n",
      "Find more information about API v2.0 and IR v11 at https://docs.openvino.ai/latest/openvino_2_0_transition_guide.html\n",
      "[ SUCCESS ] Generated IR version 11 model.\n",
      "[ SUCCESS ] XML file: /home/ISAS.DE/yu.zhou/EfficientBioAI/tutorial/academic_deploy_model.xml\n",
      "[ SUCCESS ] BIN file: /home/ISAS.DE/yu.zhou/EfficientBioAI/tutorial/academic_deploy_model.bin\n",
      "2023-05-04 18:37:31 [EFFICIENTBIOAI] INFO: transform done!\n"
     ]
    }
   ],
   "source": [
    "exp_path = Path(\"./exp/test_opv_int4\")\n",
    "Path.mkdir(exp_path, exist_ok=True)\n",
    "pipeline = Pipeline.setup(config_yml)\n",
    "pipeline(deepcopy(net), dataloader, fine_tune, calibrate, exp_path)\n",
    "pipeline.network2ir()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Infer the model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the openvino inference engine to do the inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from efficientbioai.infer.backend.openvino import create_opv_model\n",
    "from monai.inferers import sliding_window_inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = config.model.model_name\n",
    "cfg_path = exp_path / f\"{model_name}.yaml\"\n",
    "infer_path = exp_path / \"academic_deploy_model.xml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "65it [00:00, 75187.47it/s]\n"
     ]
    }
   ],
   "source": [
    "test_data_path = Path(\"./data/Fluo-N2DH-SIM+/01\")\n",
    "test_gt_path = Path(\"./data/Fluo-N2DH-SIM+/01_GT/SEG\")\n",
    "test_dataset = Dataset(\n",
    "    data=generate_data_dict(test_data_path, test_gt_path), transform=test_transform\n",
    ")\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "inference with the quantized model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_model = create_opv_model(infer_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 65/65 [00:16<00:00,  3.93it/s]\n"
     ]
    }
   ],
   "source": [
    "for i, batch_data in tenumerate(test_dataloader):\n",
    "    data, label = batch_data[\"img\"], batch_data[\"seg\"]\n",
    "    sliding_window_inference(\n",
    "        inputs=data,\n",
    "        predictor=quantized_model,\n",
    "        device=torch.device(\"cpu\"),\n",
    "        roi_size=(128, 128),\n",
    "        sw_batch_size=1,\n",
    "        overlap=0.1,\n",
    "        mode=\"constant\",\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "inference with the normal model (float32, not on the engine)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 65/65 [01:05<00:00,  1.00s/it]\n"
     ]
    }
   ],
   "source": [
    "normal_model = net\n",
    "normal_model.eval()\n",
    "for i, batch_data in tenumerate(test_dataloader):\n",
    "    data, label = batch_data[\"img\"], batch_data[\"seg\"]\n",
    "    sliding_window_inference(\n",
    "        inputs=data,\n",
    "        predictor=normal_model,\n",
    "        device=torch.device(\"cpu\"),\n",
    "        roi_size=(128, 128),\n",
    "        sw_batch_size=1,\n",
    "        overlap=0.1,\n",
    "        mode=\"constant\",\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Through compression, the inference speed is improved by 4x."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test latency and throughput using benchmark_app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 1/11] Parsing and validating input arguments\n",
      "[ INFO ] Parsing input parameters\n",
      "[Step 2/11] Loading OpenVINO Runtime\n",
      "[ INFO ] OpenVINO:\n",
      "[ INFO ] Build ................................. 2022.3.0-9052-9752fafe8eb-releases/2022/3\n",
      "[ INFO ] \n",
      "[ INFO ] Device info:\n",
      "[ INFO ] CPU\n",
      "[ INFO ] Build ................................. 2022.3.0-9052-9752fafe8eb-releases/2022/3\n",
      "[ INFO ] \n",
      "[ INFO ] \n",
      "[Step 3/11] Setting device configuration\n",
      "[ WARNING ] Performance hint was not explicitly specified in command line. Device(CPU) performance hint will be set to THROUGHPUT.\n",
      "[Step 4/11] Reading model files\n",
      "[ INFO ] Loading model files\n",
      "[ INFO ] Read model took 27.69 ms\n",
      "[ INFO ] Original model I/O parameters:\n",
      "[ INFO ] Model inputs:\n",
      "[ INFO ]     input (node: input) : f32 / [...] / [?,1,128,128]\n",
      "[ INFO ] Model outputs:\n",
      "[ INFO ]     output (node: output) : f32 / [...] / [?,2,128,128]\n",
      "[Step 5/11] Resizing model to match image sizes and given batch\n",
      "[ INFO ] Model batch size: ?\n",
      "[ INFO ] Reshaping model: 'input': [?,1,128,128]\n",
      "[ INFO ] Reshape model took 1.45 ms\n",
      "[Step 6/11] Configuring input of the model\n",
      "[ INFO ] Model inputs:\n",
      "[ INFO ]     input (node: input) : f32 / [N,C,H,W] / [?,1,128,128]\n",
      "[ INFO ] Model outputs:\n",
      "[ INFO ]     output (node: output) : f32 / [...] / [?,2,128,128]\n",
      "[Step 7/11] Loading the model to the device\n",
      "[ INFO ] Compile model took 155.23 ms\n",
      "[Step 8/11] Querying optimal runtime parameters\n",
      "[ INFO ] Model:\n",
      "[ INFO ]   NETWORK_NAME: torch-jit-export\n",
      "[ INFO ]   OPTIMAL_NUMBER_OF_INFER_REQUESTS: 4\n",
      "[ INFO ]   NUM_STREAMS: 4\n",
      "[ INFO ]   AFFINITY: Affinity.CORE\n",
      "[ INFO ]   INFERENCE_NUM_THREADS: 16\n",
      "[ INFO ]   PERF_COUNT: False\n",
      "[ INFO ]   INFERENCE_PRECISION_HINT: <Type: 'float32'>\n",
      "[ INFO ]   PERFORMANCE_HINT: PerformanceMode.THROUGHPUT\n",
      "[ INFO ]   PERFORMANCE_HINT_NUM_REQUESTS: 0\n",
      "[Step 9/11] Creating infer requests and preparing input tensors\n",
      "[ WARNING ] No input files were given for input 'input'!. This input will be filled with random values!\n",
      "[ INFO ] Fill input 'input' with random values \n",
      "[Step 10/11] Measuring performance (Start inference asynchronously, 4 inference requests, limits: 15000 ms duration)\n",
      "[ INFO ] Benchmarking in full mode (inputs filling are included in measurement loop).\n",
      "[ INFO ] First inference took 90.49 ms\n",
      "[Step 11/11] Dumping statistics report\n",
      "[ INFO ] Count:            527 iterations\n",
      "[ INFO ] Duration:         15108.43 ms\n",
      "[ INFO ] Latency:\n",
      "[ INFO ]    Median:        113.88 ms\n",
      "[ INFO ]    Average:       114.32 ms\n",
      "[ INFO ]    Min:           62.21 ms\n",
      "[ INFO ]    Max:           163.60 ms\n",
      "[ INFO ] Throughput:   139.52 FPS\n"
     ]
    }
   ],
   "source": [
    "!benchmark_app -m \"exp/test_opv_int8/academic_deploy_model.xml\" -d CPU -api async -t 15 -shape [-1,1,128,128] -data_shape [4,1,128,128]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 1/11] Parsing and validating input arguments\n",
      "[ INFO ] Parsing input parameters\n",
      "[Step 2/11] Loading OpenVINO Runtime\n",
      "[ INFO ] OpenVINO:\n",
      "[ INFO ] Build ................................. 2022.3.0-9052-9752fafe8eb-releases/2022/3\n",
      "[ INFO ] \n",
      "[ INFO ] Device info:\n",
      "[ INFO ] CPU\n",
      "[ INFO ] Build ................................. 2022.3.0-9052-9752fafe8eb-releases/2022/3\n",
      "[ INFO ] \n",
      "[ INFO ] \n",
      "[Step 3/11] Setting device configuration\n",
      "[ WARNING ] Performance hint was not explicitly specified in command line. Device(CPU) performance hint will be set to THROUGHPUT.\n",
      "[Step 4/11] Reading model files\n",
      "[ INFO ] Loading model files\n",
      "[ INFO ] Read model took 19.15 ms\n",
      "[ INFO ] Original model I/O parameters:\n",
      "[ INFO ] Model inputs:\n",
      "[ INFO ]     input (node: input) : f32 / [...] / [?,1,128,128]\n",
      "[ INFO ] Model outputs:\n",
      "[ INFO ]     output (node: output) : f32 / [...] / [?,2,128,128]\n",
      "[Step 5/11] Resizing model to match image sizes and given batch\n",
      "[ INFO ] Model batch size: ?\n",
      "[ INFO ] Reshaping model: 'input': [?,1,128,128]\n",
      "[ INFO ] Reshape model took 0.61 ms\n",
      "[Step 6/11] Configuring input of the model\n",
      "[ INFO ] Model inputs:\n",
      "[ INFO ]     input (node: input) : f32 / [N,C,H,W] / [?,1,128,128]\n",
      "[ INFO ] Model outputs:\n",
      "[ INFO ]     output (node: output) : f32 / [...] / [?,2,128,128]\n",
      "[Step 7/11] Loading the model to the device\n",
      "[ INFO ] Compile model took 107.99 ms\n",
      "[Step 8/11] Querying optimal runtime parameters\n",
      "[ INFO ] Model:\n",
      "[ INFO ]   NETWORK_NAME: torch-jit-export\n",
      "[ INFO ]   OPTIMAL_NUMBER_OF_INFER_REQUESTS: 4\n",
      "[ INFO ]   NUM_STREAMS: 4\n",
      "[ INFO ]   AFFINITY: Affinity.CORE\n",
      "[ INFO ]   INFERENCE_NUM_THREADS: 16\n",
      "[ INFO ]   PERF_COUNT: False\n",
      "[ INFO ]   INFERENCE_PRECISION_HINT: <Type: 'float32'>\n",
      "[ INFO ]   PERFORMANCE_HINT: PerformanceMode.THROUGHPUT\n",
      "[ INFO ]   PERFORMANCE_HINT_NUM_REQUESTS: 0\n",
      "[Step 9/11] Creating infer requests and preparing input tensors\n",
      "[ WARNING ] No input files were given for input 'input'!. This input will be filled with random values!\n",
      "[ INFO ] Fill input 'input' with random values \n",
      "[Step 10/11] Measuring performance (Start inference asynchronously, 4 inference requests, limits: 15000 ms duration)\n",
      "[ INFO ] Benchmarking in full mode (inputs filling are included in measurement loop).\n",
      "[ INFO ] First inference took 122.18 ms\n",
      "[Step 11/11] Dumping statistics report\n",
      "[ INFO ] Count:            325 iterations\n",
      "[ INFO ] Duration:         15168.59 ms\n",
      "[ INFO ] Latency:\n",
      "[ INFO ]    Median:        185.83 ms\n",
      "[ INFO ]    Average:       186.12 ms\n",
      "[ INFO ]    Min:           99.31 ms\n",
      "[ INFO ]    Max:           237.86 ms\n",
      "[ INFO ] Throughput:   85.70 FPS\n"
     ]
    }
   ],
   "source": [
    "!benchmark_app -m \"exp/test_opv_fp32/academic_deploy_model.xml\" -d CPU -api async -t 15 -shape [-1,1,128,128] -data_shape [4,1,128,128]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 1/11] Parsing and validating input arguments\n",
      "[ INFO ] Parsing input parameters\n",
      "[Step 2/11] Loading OpenVINO Runtime\n",
      "[ INFO ] OpenVINO:\n",
      "[ INFO ] Build ................................. 2022.3.0-9052-9752fafe8eb-releases/2022/3\n",
      "[ INFO ] \n",
      "[ INFO ] Device info:\n",
      "[ INFO ] CPU\n",
      "[ INFO ] Build ................................. 2022.3.0-9052-9752fafe8eb-releases/2022/3\n",
      "[ INFO ] \n",
      "[ INFO ] \n",
      "[Step 3/11] Setting device configuration\n",
      "[ WARNING ] Performance hint was not explicitly specified in command line. Device(CPU) performance hint will be set to THROUGHPUT.\n",
      "[Step 4/11] Reading model files\n",
      "[ INFO ] Loading model files\n",
      "[ INFO ] Read model took 13.50 ms\n",
      "[ INFO ] Original model I/O parameters:\n",
      "[ INFO ] Model inputs:\n",
      "[ INFO ]     input (node: input) : f32 / [...] / [?,1,128,128]\n",
      "[ INFO ] Model outputs:\n",
      "[ INFO ]     output (node: output) : f32 / [...] / [?,2,128,128]\n",
      "[Step 5/11] Resizing model to match image sizes and given batch\n",
      "[ INFO ] Model batch size: ?\n",
      "[ INFO ] Reshaping model: 'input': [?,1,128,128]\n",
      "[ INFO ] Reshape model took 1.02 ms\n",
      "[Step 6/11] Configuring input of the model\n",
      "[ INFO ] Model inputs:\n",
      "[ INFO ]     input (node: input) : f32 / [N,C,H,W] / [?,1,128,128]\n",
      "[ INFO ] Model outputs:\n",
      "[ INFO ]     output (node: output) : f32 / [...] / [?,2,128,128]\n",
      "[Step 7/11] Loading the model to the device\n",
      "[ INFO ] Compile model took 154.06 ms\n",
      "[Step 8/11] Querying optimal runtime parameters\n",
      "[ INFO ] Model:\n",
      "[ INFO ]   NETWORK_NAME: torch-jit-export\n",
      "[ INFO ]   OPTIMAL_NUMBER_OF_INFER_REQUESTS: 4\n",
      "[ INFO ]   NUM_STREAMS: 4\n",
      "[ INFO ]   AFFINITY: Affinity.CORE\n",
      "[ INFO ]   INFERENCE_NUM_THREADS: 16\n",
      "[ INFO ]   PERF_COUNT: False\n",
      "[ INFO ]   INFERENCE_PRECISION_HINT: <Type: 'float32'>\n",
      "[ INFO ]   PERFORMANCE_HINT: PerformanceMode.THROUGHPUT\n",
      "[ INFO ]   PERFORMANCE_HINT_NUM_REQUESTS: 0\n",
      "[Step 9/11] Creating infer requests and preparing input tensors\n",
      "[ WARNING ] No input files were given for input 'input'!. This input will be filled with random values!\n",
      "[ INFO ] Fill input 'input' with random values \n",
      "[Step 10/11] Measuring performance (Start inference asynchronously, 4 inference requests, limits: 15000 ms duration)\n",
      "[ INFO ] Benchmarking in full mode (inputs filling are included in measurement loop).\n",
      "[ INFO ] First inference took 85.79 ms\n",
      "[Step 11/11] Dumping statistics report\n",
      "[ INFO ] Count:            533 iterations\n",
      "[ INFO ] Duration:         15105.25 ms\n",
      "[ INFO ] Latency:\n",
      "[ INFO ]    Median:        112.59 ms\n",
      "[ INFO ]    Average:       112.98 ms\n",
      "[ INFO ]    Min:           58.02 ms\n",
      "[ INFO ]    Max:           166.53 ms\n",
      "[ INFO ] Throughput:   141.14 FPS\n"
     ]
    }
   ],
   "source": [
    "!benchmark_app -m \"exp/test_opv_int4/academic_deploy_model.xml\" -d CPU -api async -t 15 -shape [-1,1,128,128] -data_shape [4,1,128,128]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yz_deployment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ae4f827bcde0c94ad1c0f2596c29b2a24729759a5f96d744c12fba254054871a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
