mode: train

data:
  category: "pair"
  data_path: '/mnt/data/ISAS.DE/yu.zhou/Yu/sandbox/EfficientBioAI/data/mmv_im2im/semanticseg/train'
  dataloader:
    train:
      dataloader_params:
        batch_size: 1
        pin_memory: True
        num_workers: 4

  preprocess:
    - module_name: monai.transforms
      func_name: LoadImaged
      params:
        keys: ["IM"]
        dimension_order_out: "ZYX"
        C: 0
        T: 0
    - module_name: monai.transforms
      func_name: LoadImaged
      params:
        keys: ["GT"]
        dtype: int
        dimension_order_out: "ZYX"
        C: 0
        T: 0
    - module_name: monai.transforms
      func_name: AddChanneld
      params:
        keys: ["IM", "GT"]
    - module_name: monai.transforms
      func_name: NormalizeIntensityd
      params:
        keys: ["IM"]
    - module_name: monai.transforms
      func_name: RandSpatialCropSamplesd
      params:
        keys: ["IM", "GT"]
        random_size: False
        num_samples: 1
        roi_size: [32, 128, 128]
    - module_name: monai.transforms
      func_name: EnsureTyped
      params:
        keys: ["IM", "GT"]
  augmentation:
    - module_name: monai.transforms
      func_name: RandFlipd
      params:
        prob: 0.5
        keys: ["IM", "GT"]

model:
  framework: FCN
  checkpoint: model/mmv_im2im/semanticseg_3d.ckpt
  net:
    module_name: monai.networks.nets
    func_name: DynUnet
    params:
      spatial_dims: 3
      in_channels: 1
      out_channels: 2
      strides: [[1, 1, 1], [1, 2, 2], [1, 1, 1], [2, 2, 2]]
      kernel_size: [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]]
      upsample_kernel_size:  [[1, 2, 2], [1, 1, 1], [2, 2, 2]]
      filters: [64, 128, 192, 256]
      dropout: 0.2
      res_block: True
  criterion:
    module_name: monai.losses
    func_name: GeneralizedDiceFocalLoss
    params:
      softmax: True
      to_onehot_y: True
  optimizer:
    module_name: torch.optim
    func_name: Adam  # AdamW
    params:
      lr: 0.001
      weight_decay: 0.0005
  scheduler:
    module_name: torch.optim.lr_scheduler
    func_name: ExponentialLR
    params:
      gamma: 0.98
trainer:
  verbose: True
  params:
    accelerator: "gpu"
    devices: 1
    precision: 16
    max_epochs: 100